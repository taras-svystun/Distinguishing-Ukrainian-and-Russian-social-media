{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0545a27d",
   "metadata": {},
   "source": [
    "<h1><center>This is a notebook, which illustrates the ideas behind main NLP preprocessing techniques</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a00bcd4",
   "metadata": {},
   "source": [
    "<h1><center>The code is made by Taras Svystun</center></h1>\n",
    "tatarik.sv@gmail.com\n",
    "\n",
    "https://github.com/taras-svystun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ba78f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_md\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from collections import OrderedDict\n",
    "from random import shuffle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c06926",
   "metadata": {},
   "source": [
    "## Part-Of-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89469805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('High', 'ADJ'), ('above', 'ADP'), ('the', 'DET'), ('city', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "nlp = en_core_web_md.load()\n",
    "doc = nlp('High above the city.')\n",
    "POS_tagging = [(token.text, token.pos_) for token in doc]\n",
    "print(POS_tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb78d5fa",
   "metadata": {},
   "source": [
    "## Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc85760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Taras', 'PERSON'), ('UCU', 'ORG'), ('Europe', 'LOC')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Taras claims, UCU is the best university in Europe')\n",
    "NER = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2751c1",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80ece309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'Ford', 'like', 'like', 'like', 'Ford']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('I liked my Ford 67, but now I like and likes and liking Ford 69.')\n",
    "lemmas = [token.lemma_ for token in doc if \n",
    "          not token.is_stop and \n",
    "          not token.is_punct and \n",
    "          not token.is_digit]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed9990e",
   "metadata": {},
   "source": [
    "## Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c80572",
   "metadata": {},
   "source": [
    "## Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82ac245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(*sentences):\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        lemmas = [token.lemma_ for token in doc if \n",
    "          not token.is_stop and \n",
    "          not token.is_punct and \n",
    "          not token.is_digit]\n",
    "        yield lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c78f9754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(lemmas):\n",
    "    vocab = set()\n",
    "    for lemma in lemmas:\n",
    "        unique_lemma = set(lemma)\n",
    "        vocab = vocab.union(unique_lemma)\n",
    "    return list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fee01832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2bow(lemmas, vocab):\n",
    "    for lemma in lemmas:\n",
    "        yield [lemma.count(word) for word in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01471c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'eat', 'produce', 'bark', 'milk', 'cow', 'grass']\n",
      "[0, 1, 1, 0, 1, 1, 1]\n",
      "[2, 0, 0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "sent1 = 'The cow eats grass and then produces milk.'\n",
    "sent2 = 'The dog with other dogs bark at the cow.'\n",
    "\n",
    "lemmas = list(lemmatize(sent1, sent2))\n",
    "vocab = make_vocab(lemmas)\n",
    "print(vocab)\n",
    "for bow in doc2bow(lemmas, vocab):\n",
    "    print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d75fa8",
   "metadata": {},
   "source": [
    "Just another way to create BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0492136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow(texts):\n",
    "    dictionary = OrderedDict()\n",
    "    for text in texts:\n",
    "        for word in text:\n",
    "            if word in dictionary:\n",
    "                dictionary[word] += 1\n",
    "            else:\n",
    "                dictionary[word] = 1\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e85230",
   "metadata": {},
   "source": [
    "## TF-iDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd61c09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Football club club club Arsenal defeat local rivals this weekend.\",\n",
    "             \"Weekend football frenzy takes over London.\",\n",
    "             \"Bank open for take over bids after losing millions.\",\n",
    "             \"London football clubs bid to move to Wembley stadium.\",\n",
    "             \"Arsenal bid 50 million pounds bid bid bid for striker Kane.\",\n",
    "             \"Financial troubles result in loss of millions for bank.\",\n",
    "             \"Western bank files for bankruptcy after financial losses.\",\n",
    "             \"London football club is taken over by oil millionaire from Russia.\",\n",
    "             \"Banking on finances not working for Russia.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30ac73c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['football', 'club', 'club', 'club', 'Arsenal', 'defeat', 'local', 'rival', 'weekend']\n",
      "['Weekend', 'football', 'frenzy', 'take', 'London']\n",
      "['bank', 'open', 'bid', 'lose', 'million']\n",
      "['London', 'football', 'club', 'bid', 'Wembley', 'stadium']\n",
      "['arsenal', 'bid', 'million', 'pound', 'bid', 'bid', 'bid', 'striker', 'Kane']\n",
      "['financial', 'trouble', 'result', 'loss', 'million', 'bank']\n",
      "['western', 'bank', 'file', 'bankruptcy', 'financial', 'loss']\n",
      "['London', 'football', 'club', 'take', 'oil', 'millionaire', 'Russia']\n",
      "['banking', 'finance', 'work', 'Russia']\n"
     ]
    }
   ],
   "source": [
    "texts = list(lemmatize(*documents))\n",
    "for text in texts:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14137cf",
   "metadata": {},
   "source": [
    "### Test our manual implementation of Dictionary vs. Gensim\n",
    "1. Make dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60ceff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dictionary2(texts):\n",
    "    index = 0\n",
    "    result = {}\n",
    "    for text in texts:\n",
    "        for word in text:\n",
    "            if word not in result:\n",
    "                result[word] = index\n",
    "                index += 1\n",
    "    return result\n",
    "\n",
    "def make_dictionary(texts):\n",
    "    unique_words = {word for text in texts for word in text}\n",
    "    return dict(zip(unique_words, range(len(unique_words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de0e5917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'striker': 0, 'result': 1, 'pound': 2, 'football': 3, 'bankruptcy': 4, 'oil': 5, 'local': 6, 'work': 7, 'Weekend': 8, 'western': 9, 'Wembley': 10, 'club': 11, 'open': 12, 'millionaire': 13, 'Russia': 14, 'million': 15, 'financial': 16, 'bank': 17, 'London': 18, 'loss': 19, 'take': 20, 'finance': 21, 'lose': 22, 'Arsenal': 23, 'trouble': 24, 'weekend': 25, 'stadium': 26, 'defeat': 27, 'rival': 28, 'arsenal': 29, 'file': 30, 'banking': 31, 'Kane': 32, 'frenzy': 33, 'bid': 34}\n"
     ]
    }
   ],
   "source": [
    "dictionary = make_dictionary(texts)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca39976",
   "metadata": {},
   "source": [
    "##### vs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c675cb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Arsenal': 0, 'club': 1, 'defeat': 2, 'football': 3, 'local': 4, 'rival': 5, 'weekend': 6, 'London': 7, 'Weekend': 8, 'frenzy': 9, 'take': 10, 'bank': 11, 'bid': 12, 'lose': 13, 'million': 14, 'open': 15, 'Wembley': 16, 'stadium': 17, 'Kane': 18, 'arsenal': 19, 'pound': 20, 'striker': 21, 'financial': 22, 'loss': 23, 'result': 24, 'trouble': 25, 'bankruptcy': 26, 'file': 27, 'western': 28, 'Russia': 29, 'millionaire': 30, 'oil': 31, 'banking': 32, 'finance': 33, 'work': 34}\n"
     ]
    }
   ],
   "source": [
    "dictionary2 = corpora.Dictionary(texts)\n",
    "print(dictionary2.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c34d8c",
   "metadata": {},
   "source": [
    "2. Transform texts to BOW according to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9aaa38c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_doc2bow(texts, dictionary):\n",
    "    result = []\n",
    "    for text in texts:\n",
    "        bow = []\n",
    "        for word in set(text):\n",
    "            bow.append((dictionary[word], text.count(word)))\n",
    "        result.append(bow)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ca188b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(23, 1), (3, 1), (11, 3), (25, 1), (6, 1), (27, 1), (28, 1)]\n",
      "[(18, 1), (20, 1), (33, 1), (3, 1), (8, 1)]\n",
      "[(17, 1), (22, 1), (12, 1), (15, 1), (34, 1)]\n",
      "[(18, 1), (10, 1), (3, 1), (11, 1), (26, 1), (34, 1)]\n",
      "[(0, 1), (32, 1), (2, 1), (15, 1), (34, 4), (29, 1)]\n",
      "[(16, 1), (17, 1), (1, 1), (24, 1), (15, 1), (19, 1)]\n",
      "[(16, 1), (9, 1), (17, 1), (4, 1), (19, 1), (30, 1)]\n",
      "[(18, 1), (20, 1), (3, 1), (11, 1), (14, 1), (13, 1), (5, 1)]\n",
      "[(31, 1), (7, 1), (14, 1), (21, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = dict_doc2bow(texts, dictionary)\n",
    "print('\\n'.join(map(str, corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9f73f6",
   "metadata": {},
   "source": [
    "#### vs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d78f2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(23, 1), (3, 1), (11, 3), (25, 1), (6, 1), (27, 1), (28, 1)]\n",
      "[(18, 1), (20, 1), (33, 1), (3, 1), (8, 1)]\n",
      "[(17, 1), (22, 1), (12, 1), (15, 1), (34, 1)]\n",
      "[(18, 1), (10, 1), (3, 1), (11, 1), (26, 1), (34, 1)]\n",
      "[(0, 1), (32, 1), (2, 1), (15, 1), (34, 4), (29, 1)]\n",
      "[(16, 1), (17, 1), (1, 1), (24, 1), (15, 1), (19, 1)]\n",
      "[(16, 1), (9, 1), (17, 1), (4, 1), (19, 1), (30, 1)]\n",
      "[(18, 1), (20, 1), (3, 1), (11, 1), (14, 1), (13, 1), (5, 1)]\n",
      "[(31, 1), (7, 1), (14, 1), (21, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus2 = [dictionary2.doc2bow(text) for text in texts]\n",
    "print('\\n'.join(map(str, corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f8bab6",
   "metadata": {},
   "source": [
    "### Now it's time to implement TF-iDF itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc42a193",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term - 23, frequency - 0.11\n",
      "term - 3, frequency - 0.11\n",
      "term - 11, frequency - 0.33\n",
      "term - 25, frequency - 0.11\n",
      "term - 6, frequency - 0.11\n",
      "term - 27, frequency - 0.11\n",
      "term - 28, frequency - 0.11\n",
      "-----------------------------\n",
      "term - 18, frequency - 0.2\n",
      "term - 20, frequency - 0.2\n",
      "term - 33, frequency - 0.2\n",
      "term - 3, frequency - 0.2\n",
      "term - 8, frequency - 0.2\n",
      "-----------------------------\n",
      "term - 17, frequency - 0.2\n",
      "term - 22, frequency - 0.2\n",
      "term - 12, frequency - 0.2\n",
      "term - 15, frequency - 0.2\n",
      "term - 34, frequency - 0.2\n",
      "-----------------------------\n",
      "term - 18, frequency - 0.17\n",
      "term - 10, frequency - 0.17\n",
      "term - 3, frequency - 0.17\n",
      "term - 11, frequency - 0.17\n",
      "term - 26, frequency - 0.17\n",
      "term - 34, frequency - 0.17\n",
      "-----------------------------\n",
      "term - 0, frequency - 0.11\n",
      "term - 32, frequency - 0.11\n",
      "term - 2, frequency - 0.11\n",
      "term - 15, frequency - 0.11\n",
      "term - 34, frequency - 0.44\n",
      "term - 29, frequency - 0.11\n",
      "-----------------------------\n",
      "term - 16, frequency - 0.17\n",
      "term - 17, frequency - 0.17\n",
      "term - 1, frequency - 0.17\n",
      "term - 24, frequency - 0.17\n",
      "term - 15, frequency - 0.17\n",
      "term - 19, frequency - 0.17\n",
      "-----------------------------\n",
      "term - 16, frequency - 0.17\n",
      "term - 9, frequency - 0.17\n",
      "term - 17, frequency - 0.17\n",
      "term - 4, frequency - 0.17\n",
      "term - 19, frequency - 0.17\n",
      "term - 30, frequency - 0.17\n",
      "-----------------------------\n",
      "term - 18, frequency - 0.14\n",
      "term - 20, frequency - 0.14\n",
      "term - 3, frequency - 0.14\n",
      "term - 11, frequency - 0.14\n",
      "term - 14, frequency - 0.14\n",
      "term - 13, frequency - 0.14\n",
      "term - 5, frequency - 0.14\n",
      "-----------------------------\n",
      "term - 31, frequency - 0.25\n",
      "term - 7, frequency - 0.25\n",
      "term - 14, frequency - 0.25\n",
      "term - 21, frequency - 0.25\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_tf(corpus):\n",
    "    nDocuments = len(corpus)\n",
    "    TF = dict.fromkeys(range(1, nDocuments + 1), dict())\n",
    "    \n",
    "    for idx, document in enumerate(corpus):\n",
    "        total_frequency, tf = 0, dict()\n",
    "        for term, frequency in document:\n",
    "            tf[term] = tf.get(term, 0) + frequency\n",
    "            total_frequency += frequency\n",
    "\n",
    "        for term, frequency in tf.items():\n",
    "            tf[term] = frequency / total_frequency\n",
    "        TF[idx + 1] = tf\n",
    "\n",
    "    return TF\n",
    "\n",
    "TF = get_tf(corpus)\n",
    "for document in TF.values():\n",
    "    for term, freq in document.items():\n",
    "        print(f'term - {term}, frequency - {round(freq, 2)}')\n",
    "    print('-----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e947f4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{23: [1], 3: [1, 2, 4, 8], 11: [1, 4, 8], 25: [1], 6: [1], 27: [1], 28: [1], 18: [2, 4, 8], 20: [2, 8], 33: [2], 8: [2], 17: [3, 6, 7], 22: [3], 12: [3], 15: [3, 5, 6], 34: [3, 4, 5], 10: [4], 26: [4], 0: [5], 32: [5], 2: [5], 29: [5], 16: [6, 7], 1: [6], 24: [6], 19: [6, 7], 9: [7], 4: [7], 30: [7], 14: [8, 9], 13: [8], 5: [8], 31: [9], 7: [9], 21: [9]}\n"
     ]
    }
   ],
   "source": [
    "def get_term_container(corpus):\n",
    "    term_container = dict()\n",
    "    for idx, document in enumerate(corpus):\n",
    "        for term, frequency in document:\n",
    "            if term in term_container:\n",
    "                term_container[term].append(idx + 1)\n",
    "            else:\n",
    "                term_container[term] = [idx + 1]\n",
    "    return term_container\n",
    "\n",
    "term_container = get_term_container(corpus)\n",
    "print(term_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86a5dbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term - 23, inverse document frequency - 2.2\n",
      "term - 3, inverse document frequency - 0.81\n",
      "term - 11, inverse document frequency - 1.1\n",
      "term - 25, inverse document frequency - 2.2\n",
      "term - 6, inverse document frequency - 2.2\n",
      "term - 27, inverse document frequency - 2.2\n",
      "term - 28, inverse document frequency - 2.2\n",
      "term - 18, inverse document frequency - 1.1\n",
      "term - 20, inverse document frequency - 1.5\n",
      "term - 33, inverse document frequency - 2.2\n",
      "term - 8, inverse document frequency - 2.2\n",
      "term - 17, inverse document frequency - 1.1\n",
      "term - 22, inverse document frequency - 2.2\n",
      "term - 12, inverse document frequency - 2.2\n",
      "term - 15, inverse document frequency - 1.1\n",
      "term - 34, inverse document frequency - 1.1\n",
      "term - 10, inverse document frequency - 2.2\n",
      "term - 26, inverse document frequency - 2.2\n",
      "term - 0, inverse document frequency - 2.2\n",
      "term - 32, inverse document frequency - 2.2\n",
      "term - 2, inverse document frequency - 2.2\n",
      "term - 29, inverse document frequency - 2.2\n",
      "term - 16, inverse document frequency - 1.5\n",
      "term - 1, inverse document frequency - 2.2\n",
      "term - 24, inverse document frequency - 2.2\n",
      "term - 19, inverse document frequency - 1.5\n",
      "term - 9, inverse document frequency - 2.2\n",
      "term - 4, inverse document frequency - 2.2\n",
      "term - 30, inverse document frequency - 2.2\n",
      "term - 14, inverse document frequency - 1.5\n",
      "term - 13, inverse document frequency - 2.2\n",
      "term - 5, inverse document frequency - 2.2\n",
      "term - 31, inverse document frequency - 2.2\n",
      "term - 7, inverse document frequency - 2.2\n",
      "term - 21, inverse document frequency - 2.2\n"
     ]
    }
   ],
   "source": [
    "def get_idf(corpus, term_container):\n",
    "    IDF = dict()\n",
    "    nDocuments = len(corpus)\n",
    "    for term, documents_with_term in term_container.items():\n",
    "        IDF[term] = np.log(nDocuments / len(documents_with_term))\n",
    "    return IDF\n",
    "\n",
    "\n",
    "IDF = get_idf(corpus, term_container)\n",
    "for term, idf in IDF.items():\n",
    "    print(f'term - {term}, inverse document frequency - {round(idf, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94f9e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_model(corpus=None, TF=None, IDF=None):\n",
    "    nDocuments, TFIDF = len(corpus), dict()\n",
    "    \n",
    "    for idx in range(nDocuments):\n",
    "        TFIDF[idx + 1] = dict()\n",
    "\n",
    "    for idx, tf in TF.items():\n",
    "        for term, freq in tf.items():\n",
    "            TFIDF[idx][term] = freq * IDF[term]\n",
    "\n",
    "    return TFIDF\n",
    "\n",
    "TFIDF = tfidf_model(corpus=corpus, TF=TF, IDF=IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e069b3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 0.24413606414846883)\n",
      "(3, 0.09010335735736986)\n",
      "(11, 0.3662040962227032)\n",
      "(25, 0.24413606414846883)\n",
      "(6, 0.24413606414846883)\n",
      "(27, 0.24413606414846883)\n",
      "(28, 0.24413606414846883)\n"
     ]
    }
   ],
   "source": [
    "example = 0 + 1\n",
    "for term, tfidf in TFIDF[example].items():\n",
    "    print((term, tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2f71252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 0.3679502401984268)\n",
      "(3, 0.1357994858234748)\n",
      "(11, 0.5519253602976403)\n",
      "(25, 0.3679502401984268)\n",
      "(6, 0.3679502401984268)\n",
      "(27, 0.3679502401984268)\n",
      "(28, 0.3679502401984268)\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus=corpus)\n",
    "example = corpus[0]\n",
    "for term, tfidf in tfidf[example]:\n",
    "    print((term, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3068fe20",
   "metadata": {},
   "source": [
    "## n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca13cbf",
   "metadata": {},
   "source": [
    "#### Searching for bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5cd1007",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = models.Phrases(texts)\n",
    "texts = [bigram[line] for line in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57016939",
   "metadata": {},
   "source": [
    "#### An example of filtering words, leaving frequent ones, but not too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ce3b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary2.filter_extremes(no_below=20, no_above=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23aa884f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Clement', 'NN', 'B-GPE'), ('and', 'CC', 'O'), ('Mathieu', 'NNP', 'B-PERSON'), ('are', 'VBP', 'O'), ('working', 'VBG', 'O'), ('at', 'IN', 'O'), ('Apple', 'NNP', 'B-ORGANIZATION'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Clement and Mathieu are working at Apple.'\n",
    "ne_tree = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "iob_tagged = tree2conlltags(ne_tree)\n",
    "print(iob_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d8d4d86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Clement/NN)\n",
      "  and/CC\n",
      "  (PERSON Mathieu/NNP)\n",
      "  are/VBP\n",
      "  working/VBG\n",
      "  at/IN\n",
      "  (ORGANIZATION Apple/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "ne_tree = conlltags2tree(iob_tagged)\n",
    "print(ne_tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
