{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "468e24f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from pymystem3 import Mystem\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from time import time\n",
    "from IPython.display import clear_output\n",
    "from sys import getsizeof\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a96098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "UKR_CHANNELS = [\n",
    "    'Труха⚡️Украина', 'Лачен пишет', 'Украинская правда. Главное',\n",
    "    'Вы хотите как на Украине?', 'Борис Філатов', 'RAGNAROCK PRIVET',\n",
    "    'УНИАН - новости Украины | война с Россией | новини України | війна з Росією',\n",
    "    'Украина 24/7 Новости | Война | Новини', 'Быть Или',\n",
    "    'Украина Сейчас: новости, война, Россия'\n",
    "]\n",
    "\n",
    "UKR_LETTERS = ['ї', 'є', 'ґ', 'і']\n",
    "\n",
    "CHEAT_WORDS = [\n",
    "    '03', '04', '05', '1378', '2022', '3801', '3806', '4149', '4276',\n",
    "    '4279', '9521', '9842', 'akimapachev', 'amp', 'anna', 'com',\n",
    "    'daily', 'diza', 'donbass', 'epoddubny', 'https', 'index', 'me',\n",
    "    'news', 'opersvodki', 'pravda', 'rus', 'rvvoenkor', 'sashakots',\n",
    "    'ua', 'wargonzo', 'www', 'www pravda', 'мид', 'труха', 'труха украина',\n",
    "    'украина сейчас', 'pravda com', 'daily news', 'com ua', 'https www',\n",
    "    'me rvvoenkor', 'rus news', 'ua rus', 'wargonzo наш'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c157101",
   "metadata": {},
   "source": [
    "# Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fdcaea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_decorator(function):\n",
    "    \"\"\"\n",
    "    Just a decorator for printing the timings.\n",
    "    \"\"\"\n",
    "    from time import time\n",
    "    def inner(*args, **kwargs):\n",
    "        start = time()\n",
    "        result = function(*args, **kwargs)\n",
    "        elapsed_time = round(time() - start, 2)\n",
    "        output = f'{function.__name__} took {elapsed_time} seconds.'\n",
    "        print(output)\n",
    "        return result\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd735adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self, data=None):\n",
    "        \"\"\"\n",
    "        A class for the preprocessing purposes. Main methods icnludes:\n",
    "        reading, cleaning, lemmatizing and vectorizing the data.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.lemmas = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.ukr_train = None\n",
    "        self.ukr_test = None\n",
    "        self.channel_train = None\n",
    "        self.channel_test = None\n",
    "        self.percent_ukr = 0\n",
    "        self.percent_rus = 1\n",
    "        self.lemmatized = False\n",
    "        self.vectorized = False\n",
    "        self.cheat_words = CHEAT_WORDS\n",
    "    \n",
    "    @time_decorator\n",
    "    def read_data(self, filename='random_msgs.csv', sep='¶∆',\n",
    "                  header=None):\n",
    "        \"\"\"\n",
    "        Reads the csv file into 4 columns:\n",
    "        channel\n",
    "        date of publication\n",
    "        message\n",
    "        ukrainian - 1 if ukrainian channel, 0 - otherwise.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            self.data = pd.read_csv(filename, sep=sep, header=header)\n",
    "            self.data.columns = ['channel', 'date', 'msg']\n",
    "            self.data['ukrainian'] = self.data['channel'].\\\n",
    "            apply(lambda x: 1 if x in UKR_CHANNELS else 0)\n",
    "            self.data['ukrainian'] = self.data['ukrainian'].astype('int8')\n",
    "            self.data = self.data[self.data['channel'] != 'вечеряємо']\n",
    "            self.percent_ukr = self.data['ukrainian'].mean()\n",
    "            self.percent_rus = 1 - self.percent_ukr\n",
    "    \n",
    "    def get_data(self):\n",
    "        \"\"\"\n",
    "        Method to get the df.\n",
    "        \"\"\"\n",
    "        return self.data\n",
    "    \n",
    "    def get_percents_ukr_rus(self):\n",
    "        \"\"\"\n",
    "        Method to get the percentage of ukrainian and russian messages among\n",
    "        the dataset.\n",
    "        \"\"\"\n",
    "        return self.percent_ukr, self.percent_rus\n",
    "    \n",
    "    @time_decorator\n",
    "    def preprocess(self, remove_ukr_msgs=True, cut_less_than=18):\n",
    "        \"\"\"\n",
    "        This method:\n",
    "        removes short messages (with less than 18 characters);\n",
    "        removes messages with ukrainian letters.\n",
    "        \"\"\"\n",
    "        if remove_ukr_msgs:\n",
    "            for letter in UKR_LETTERS:\n",
    "                self.data = self.data[self.data['msg'].str.lower().\\\n",
    "                                        str.contains(letter) == False]\n",
    "        self.data = self.data[self.data['msg'].str.len() > cut_less_than]\n",
    "        self.data = self.data.reset_index(drop=True)\n",
    "        self.percent_ukr = self.data['ukrainian'].mean()\n",
    "        self.percent_rus = 1 - self.percent_ukr\n",
    "    \n",
    "    @time_decorator\n",
    "    def lemmatize(self, *sentences):\n",
    "        \"\"\"\n",
    "        This method has 2 usages:\n",
    "        internal; i.e. to lemmatize all messages in the dataset. Runs about 2.5\n",
    "        minutes.\n",
    "        outside; to lemmatize a given sequence of sentences.\n",
    "        \"\"\"\n",
    "        mystem = Mystem()\n",
    "        if not sentences:\n",
    "            if not self.lemmatized:\n",
    "                def preprocess_text(text):\n",
    "                    tokens = mystem.lemmatize(text.lower())\n",
    "                    text = \" \".join(tokens)\n",
    "                    return text\n",
    "\n",
    "                self.data['msg'] = self.data['msg'].apply(preprocess_text)\n",
    "                self.lemmas = self.data['msg'].copy()\n",
    "                self.lemmatized = True\n",
    "        else:\n",
    "            result = []\n",
    "            for sentence in sentences:\n",
    "                tokens = mystem.lemmatize(sentence.lower())\n",
    "                result.append(' '.join(tokens))\n",
    "            return result\n",
    "    \n",
    "    def get_lemmas(self):\n",
    "        \"\"\"\n",
    "        Method to get lemmatized messages.\n",
    "        \"\"\"\n",
    "        return self.lemmas\n",
    "    \n",
    "    def train_test_split(self, random_state=1, train_size=.8):\n",
    "        \"\"\"\n",
    "        This method clones scikit-learn train_test_split.\n",
    "        \"\"\"\n",
    "        self.X_train, self.X_test, self.ukr_train, self.ukr_test,\\\n",
    "        self.channel_train, self.channel_test = \\\n",
    "        train_test_split(\n",
    "            self.data['msg'], self.data['ukrainian'], self.data['channel'],\n",
    "            random_state=random_state, train_size=train_size\n",
    "        )\n",
    "    \n",
    "    def get_train_test_split(self):\n",
    "        \"\"\"\n",
    "        Returns the train and test part.\n",
    "        \"\"\"\n",
    "        return self.X_train, self.X_test, self.ukr_train, self.ukr_test,\\\n",
    "        self.channel_train, self.channel_test\n",
    "    \n",
    "    @time_decorator\n",
    "    def vectorize(self, ngram_range=(1,1), sublinear_tf=True, binary=False):\n",
    "        \"\"\"\n",
    "        This method creates a pipeline of CountVectorizer() and TfidfTransformer().\n",
    "        If CountVectorizer is needed - use count_transform method.\n",
    "        If TfidfVectorizer is needed - just call a tfidf_transform method.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not self.vectorized:\n",
    "                self.tfidf = Pipeline([\n",
    "                    ('vect', CountVectorizer(binary=binary, ngram_range=ngram_range)),\n",
    "                    ('tfidf', TfidfTransformer(sublinear_tf=sublinear_tf))\n",
    "                ]).fit(self.X_train)\n",
    "                self.vect = self.tfidf['vect']\n",
    "                self.vectorized = True\n",
    "        except TypeError:\n",
    "            print(\"You didn't initialize data or train_test_split.\")\n",
    "        \n",
    "    \n",
    "    def get_vectorizer(self, tfidf=True):\n",
    "        \"\"\"\n",
    "        Returns the actual vectorizer.\n",
    "        \"\"\"\n",
    "        return self.vectorizer\n",
    "    \n",
    "    @time_decorator\n",
    "    def tfidf_transform(self):\n",
    "        \"\"\"\n",
    "        Applies TfidfTransform to data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.vectorizer = self.tfidf\n",
    "            X_train = self.X_train = self.vectorizer.transform(self.X_train).T\n",
    "            X_test =  self.X_test = self.vectorizer.transform(self.X_test).T\n",
    "            return X_train, X_test\n",
    "        except AttributeError:\n",
    "            print(\"You didn't initialize read_data, train_test_split or vectorize.\")\n",
    "    \n",
    "    @time_decorator\n",
    "    def count_transform(self):\n",
    "        \"\"\"\n",
    "        Applies CountTransform to data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.vectorizer = self.vect\n",
    "            X_train = self.X_train = self.vectorizer.transform(self.X_train).asfptype().T\n",
    "            X_test = self.X_test = self.vectorizer.transform(self.X_test).asfptype().T\n",
    "            return X_train, X_test\n",
    "        except AttributeError:\n",
    "            print(\"You didn't initialize read_data, train_test_split or vectorize.\")\n",
    "    \n",
    "    @time_decorator\n",
    "    def remove_cheat_words(self, method='manual', freq_pivot=.5,\n",
    "                           cheat_words=CHEAT_WORDS):\n",
    "        \"\"\"\n",
    "        Removes cheat_words, like channel tags, social media links or\n",
    "        authors names.\n",
    "        \"\"\"\n",
    "        if method == 'manual':\n",
    "            delete_mask = np.zeros(self.X_train.shape[0], dtype=bool)\n",
    "            delete_mask[np.isin(np.array(\n",
    "                    self.vectorizer.get_feature_names_out()), cheat_words)\n",
    "            ] = True\n",
    "            self.X_train = self.X_train.T[:, ~delete_mask].T\n",
    "            self.X_test = self.X_test.T[:, ~delete_mask].T\n",
    "            self.delete_mask = delete_mask\n",
    "            self.cheat_words = np.array(\n",
    "                self.vectorizer.get_feature_names_out()\n",
    "            ).T[delete_mask]\n",
    "        else:\n",
    "            delete_mask = np.zeros(self.X_train.shape[0], dtype=bool)\n",
    "            for channel in self.channel_trainchannel_train.unique():\n",
    "                arr = self.X_train.T[self.channel_train == channel]\n",
    "                delete_mask |= np.array((np.sum(arr > 0, axis=0) / arr.shape[0]) > .5)[0]\n",
    "\n",
    "            self.X_train = self.X_train.T[:, ~delete_mask].T\n",
    "            self.X_test = self.X_test.T[:, ~delete_mask].T\n",
    "            self.delete_mask = delete_mask\n",
    "            self.cheat_words = np.array(\n",
    "                self.vectorizer.get_feature_names_out()\n",
    "            ).T[delete_mask]\n",
    "\n",
    "    def get_cheat_words(self):\n",
    "        \"\"\"\n",
    "        Returns the deleted cheat_words.\n",
    "        \"\"\"\n",
    "        return self.cheat_words\n",
    "    \n",
    "    def get_delete_mask(self):\n",
    "        \"\"\"\n",
    "        Returns the mask of cheat_words, which can be applied onto vectorizer matrix.\n",
    "        \"\"\"\n",
    "        return self.delete_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b041bf",
   "metadata": {},
   "source": [
    "# Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6765b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    \n",
    "    def __init__(self, SVD=[None, None, None]):\n",
    "        \"\"\"\n",
    "        Predictor class, which contains 3 predicting methods.\n",
    "        \"\"\"\n",
    "        self.Terms, self.S, self.Documents = SVD\n",
    "        if not self.S:\n",
    "            self.calculated_svd = False\n",
    "        else:\n",
    "            self.calculated_svd = True\n",
    "    \n",
    "    def get_SVD(self):\n",
    "        \"\"\"\n",
    "        Returns SVD if it is calculated onde.\n",
    "        \"\"\"\n",
    "        if self.calculated_svd:\n",
    "            return self.Terms, self.S, self.Documents\n",
    "        return 'You need to calculate SVD first'\n",
    "    \n",
    "    @time_decorator\n",
    "    def train_LSA(self, X_train, ukr_train, k=150):\n",
    "        \"\"\"\n",
    "        Calculates the SVD and then finds the centre of ukrainian and russian\n",
    "        clouds.\n",
    "        \"\"\"\n",
    "        if not self.calculated_svd:\n",
    "            self.Terms, self.S, self.Documents = svds(X_train, k=k)\n",
    "            self.ukr_centre = np.array([np.mean(self.Documents.T[ukr_train == 1], axis=0)])\n",
    "            self.rus_centre = np.array([np.mean(self.Documents.T[ukr_train == 0], axis=0)])\n",
    "            self.calculated_svd = True\n",
    "    \n",
    "    @time_decorator\n",
    "    def predict_LSA(self, X_pred):\n",
    "        \"\"\"\n",
    "        Projects X_pred onto orthonormal basis Terms and then scales in each axis by S.\n",
    "        \"\"\"\n",
    "        Documents_pred = np.diag(1 / self.S) @ self.Terms.T @ X_pred\n",
    "        dist_to_ukr = cdist(self.ukr_centre, Documents_pred.T, metric='euclidean')[0]\n",
    "        dist_to_rus = cdist(self.rus_centre, Documents_pred.T, metric='euclidean')[0]\n",
    "        ukr_pred = self.ukr_pred = np.array([dist_to_ukr < dist_to_rus]).reshape((-1, 1))\n",
    "        return ukr_pred\n",
    "    \n",
    "    def evaluate(self, ukr_test):\n",
    "        \"\"\"\n",
    "        Method to evaluate the prediction. Returns the ratio between correct guesses and\n",
    "        total.\n",
    "        \"\"\"\n",
    "        ukr_test = np.array(ukr_test).astype(bool).reshape((-1, 1))\n",
    "        self.accuracy = round(100 * np.sum(self.ukr_pred == ukr_test) / len(ukr_test), 2)\n",
    "        return self.accuracy\n",
    "    \n",
    "    def train_NBC(self, X_train, ukr_train, percent_ukr=0):\n",
    "        \"\"\"\n",
    "        Naive Bayes Classifier. Need to use only CountVectorizer(binary=True)\n",
    "        for calculating the relative frequency.\n",
    "        \"\"\"\n",
    "        self.terms_prob_ukr = np.mean(X_train.T[ukr_train == 1], axis=0)\n",
    "        self.terms_prob_rus = np.mean(X_train.T[ukr_train == 0], axis=0)\n",
    "        self.percent_ukr = percent_ukr\n",
    "        self.percent_rus = 1 - percent_ukr\n",
    "    \n",
    "    def predict_NBC(self, X_pred):\n",
    "        \"\"\"\n",
    "        A method, which predicts, using trained Naive Bayes Classifier.\n",
    "        \"\"\"\n",
    "        self.ukr_prob = self.percent_ukr * X_pred.T * self.terms_prob_ukr.T\n",
    "        self.rus_prob = self.percent_rus * X_pred.T * self.terms_prob_rus.T\n",
    "        ukr_pred = self.ukr_pred = self.ukr_prob > self.rus_prob\n",
    "\n",
    "    def train_LR(self, X_train, ukr_train):\n",
    "        \"\"\"\n",
    "        Trains Logistic Regressin.\n",
    "        \"\"\"\n",
    "        self.logistic_regression = LogisticRegression(random_state=1).fit(X_train.T, ukr_train)\n",
    "    \n",
    "    def predict_LR(self, X_pred):\n",
    "        \"\"\"\n",
    "        Predicts, using pre-trained logistic model.\n",
    "        \"\"\"\n",
    "        ukr_pred = self.ukr_pred = np.array([self.logistic_regression.predict(X_pred.T)]).reshape((-1, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
